{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T06:09:28.618120Z","iopub.execute_input":"2024-04-07T06:09:28.619234Z","iopub.status.idle":"2024-04-07T06:09:29.783188Z","shell.execute_reply.started":"2024-04-07T06:09:28.619186Z","shell.execute_reply":"2024-04-07T06:09:29.782217Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-dataset/training.1600000.processed.noemoticon.csv\n/kaggle/input/sentiment-analysis-dataset/train.csv\n/kaggle/input/sentiment-analysis-dataset/testdata.manual.2009.06.14.csv\n/kaggle/input/sentiment-analysis-dataset/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nfrom textblob import TextBlob","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:11:40.281758Z","iopub.execute_input":"2024-04-07T06:11:40.282373Z","iopub.status.idle":"2024-04-07T06:11:51.746603Z","shell.execute_reply.started":"2024-04-07T06:11:40.282334Z","shell.execute_reply":"2024-04-07T06:11:51.745567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Deep Convolutional Neural Network for Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"# load text\n# load text\nfilename = '/kaggle/input/sentiment-analysis-dataset/train.csv'\nwith open(filename, 'rt', encoding='ISO-8859-1') as file:\n    text = file.read()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:15:41.438338Z","iopub.execute_input":"2024-04-07T06:15:41.438768Z","iopub.status.idle":"2024-04-07T06:15:41.453612Z","shell.execute_reply.started":"2024-04-07T06:15:41.438739Z","shell.execute_reply":"2024-04-07T06:15:41.452106Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import string\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Assuming NLTK stopwords are already downloaded\nnltk.download('stopwords')\n\ndef clean_doc(doc):\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation from each token\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    # remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    # filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    # filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:26:58.587620Z","iopub.execute_input":"2024-04-07T06:26:58.587975Z","iopub.status.idle":"2024-04-07T06:26:58.682923Z","shell.execute_reply.started":"2024-04-07T06:26:58.587949Z","shell.execute_reply":"2024-04-07T06:26:58.681901Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"For csv file","metadata":{}},{"cell_type":"code","source":"import csv\nfrom collections import Counter\n\n# Function to add words from a document to the vocabulary\ndef add_doc_to_vocab(text, vocab):\n    words = text.split()\n    vocab.update(words)\n\n# Function to process CSV files\ndef process_csv(filename, vocab, is_train):\n    with open(filename, 'r', encoding='ISO-8859-1') as file:\n        reader = csv.reader(file)\n        # Skip header if present\n        next(reader, None)\n        for row in reader:\n            text = row[1]  # Assuming the text is in the second column, adjust as necessary\n            if is_train and row[0] == 'cv9':\n                continue\n            if not is_train and row[0] != 'cv9':\n                continue\n            add_doc_to_vocab(text, vocab)\n\n# Define vocab\nvocab = Counter()\n# Add all docs to vocab\nprocess_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', vocab, is_train=True)\n# Print the size of the vocab\nprint(\"Size of the vocabulary:\", len(vocab))\n# Print the top words in the vocab\nprint(\"Top 50 words in the vocabulary:\", vocab.most_common(50))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:30:25.508758Z","iopub.execute_input":"2024-04-07T06:30:25.510239Z","iopub.status.idle":"2024-04-07T06:30:25.694628Z","shell.execute_reply.started":"2024-04-07T06:30:25.510172Z","shell.execute_reply":"2024-04-07T06:30:25.693252Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Size of the vocabulary: 52270\nTop 50 words in the vocabulary: [('to', 9809), ('I', 8802), ('the', 8388), ('a', 6501), ('my', 4932), ('and', 4677), ('i', 4263), ('you', 3808), ('is', 3670), ('for', 3575), ('in', 3554), ('of', 3091), ('it', 3024), ('on', 2623), ('have', 2377), ('that', 2150), ('me', 2140), ('so', 2134), ('with', 2011), ('be', 1923), ('but', 1918), ('at', 1741), ('was', 1731), ('just', 1719), ('I`m', 1696), ('not', 1516), ('get', 1373), ('all', 1351), ('this', 1290), ('are', 1278), ('out', 1265), ('like', 1258), ('day', 1165), ('-', 1147), ('up', 1147), ('go', 1101), ('your', 1084), ('good', 1032), ('got', 961), ('from', 958), ('do', 934), ('going', 911), ('no', 904), ('now', 903), ('love', 880), ('work', 837), ('****', 796), ('will', 794), ('about', 785), ('one', 775)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# keep tokens with a min occurrence\nmin_occurrence = 2\ntokens = [k for k, c in vocab.items() if c >= min_occurrence]\nprint(len(tokens))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:30:59.293047Z","iopub.execute_input":"2024-04-07T06:30:59.294379Z","iopub.status.idle":"2024-04-07T06:30:59.307713Z","shell.execute_reply.started":"2024-04-07T06:30:59.294319Z","shell.execute_reply":"2024-04-07T06:30:59.305843Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"14860\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Train Embedding Layer","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:34:27.892645Z","iopub.execute_input":"2024-04-07T06:34:27.893056Z","iopub.status.idle":"2024-04-07T06:34:27.900002Z","shell.execute_reply.started":"2024-04-07T06:34:27.893024Z","shell.execute_reply":"2024-04-07T06:34:27.898276Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Function to process CSV files\ndef process_csv(filename, vocab, is_train):\n    df = pd.read_csv(filename)\n    documents = []\n    for index, row in df.iterrows():\n        text = row['text']  # Assuming the text is in a column named 'text', adjust as necessary\n        if is_train and row['label'] == 'cv9':\n            continue\n        if not is_train and row['label'] != 'cv9':\n            continue\n        documents.append(text)\n        add_doc_to_vocab(text, vocab)\n    return documents","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:34:46.749723Z","iopub.execute_input":"2024-04-07T06:34:46.750213Z","iopub.status.idle":"2024-04-07T06:34:46.758119Z","shell.execute_reply.started":"2024-04-07T06:34:46.750174Z","shell.execute_reply":"2024-04-07T06:34:46.756769Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def process_csv(filename, vocab, is_train):\n    try:\n        df = pd.read_csv(filename, encoding='ISO-8859-1')  # Try a different encoding\n    except UnicodeDecodeError:\n        df = pd.read_csv(filename, encoding='latin1')  # Try another encoding if the first one fails\n    documents = []\n    for index, row in df.iterrows():\n        text = row['text']  # Assuming the text is in a column named 'text', adjust as necessary\n        textID = row['textID']  # Assuming the textID is in a column named 'textID'\n        # Check if text is not NaN\n        if pd.notnull(text):\n            if is_train and textID == 'cv9':\n                continue\n            if not is_train and textID != 'cv9':\n                continue\n            documents.append(text)\n            add_doc_to_vocab(text, vocab)\n    return documents","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:38:15.938813Z","iopub.execute_input":"2024-04-07T06:38:15.940274Z","iopub.status.idle":"2024-04-07T06:38:15.947535Z","shell.execute_reply.started":"2024-04-07T06:38:15.940217Z","shell.execute_reply":"2024-04-07T06:38:15.946085Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# load all training reviews\npositive_docs = process_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', vocab, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:38:19.131731Z","iopub.execute_input":"2024-04-07T06:38:19.132118Z","iopub.status.idle":"2024-04-07T06:38:20.854280Z","shell.execute_reply.started":"2024-04-07T06:38:19.132088Z","shell.execute_reply":"2024-04-07T06:38:20.853095Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file into df with a different encoding\ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Now you can use df\nytrain = np.array([0 if label == 'negative' else 1 for label in df['textID']])","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:40:37.769902Z","iopub.execute_input":"2024-04-07T06:40:37.770553Z","iopub.status.idle":"2024-04-07T06:40:37.867109Z","shell.execute_reply.started":"2024-04-07T06:40:37.770520Z","shell.execute_reply":"2024-04-07T06:40:37.865179Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# create the tokenizer\ntokenizer = Tokenizer()\n# fit the tokenizer on the documents\ntokenizer.fit_on_texts(positive_docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:40:55.900003Z","iopub.execute_input":"2024-04-07T06:40:55.900564Z","iopub.status.idle":"2024-04-07T06:40:56.435690Z","shell.execute_reply.started":"2024-04-07T06:40:55.900540Z","shell.execute_reply":"2024-04-07T06:40:56.434341Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# sequence encode\nencoded_docs_train = tokenizer.texts_to_sequences(positive_docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:41:08.227709Z","iopub.execute_input":"2024-04-07T06:41:08.228197Z","iopub.status.idle":"2024-04-07T06:41:08.624061Z","shell.execute_reply.started":"2024-04-07T06:41:08.228139Z","shell.execute_reply":"2024-04-07T06:41:08.622956Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# pad sequences\nmax_length = max([len(s.split()) for s in positive_docs])\nXtrain = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:41:18.838838Z","iopub.execute_input":"2024-04-07T06:41:18.840240Z","iopub.status.idle":"2024-04-07T06:41:18.933601Z","shell.execute_reply.started":"2024-04-07T06:41:18.840174Z","shell.execute_reply":"2024-04-07T06:41:18.932242Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# load all test reviews\nnegative_docs = process_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', vocab, False)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:42:35.639818Z","iopub.execute_input":"2024-04-07T06:42:35.640177Z","iopub.status.idle":"2024-04-07T06:42:37.108434Z","shell.execute_reply.started":"2024-04-07T06:42:35.640129Z","shell.execute_reply":"2024-04-07T06:42:37.106637Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file into df with a different encoding\ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Now you can use df\nytrain = np.array([0 if label == 'negative' else 1 for label in df['textID']])","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:42:42.188054Z","iopub.execute_input":"2024-04-07T06:42:42.188756Z","iopub.status.idle":"2024-04-07T06:42:42.283483Z","shell.execute_reply.started":"2024-04-07T06:42:42.188708Z","shell.execute_reply":"2024-04-07T06:42:42.282402Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# sequence encode\nencoded_docs_test = tokenizer.texts_to_sequences(negative_docs)\n# pad sequences\nXtest = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:43:18.020133Z","iopub.execute_input":"2024-04-07T06:43:18.020558Z","iopub.status.idle":"2024-04-07T06:43:18.025714Z","shell.execute_reply.started":"2024-04-07T06:43:18.020530Z","shell.execute_reply":"2024-04-07T06:43:18.024842Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:43:30.988055Z","iopub.execute_input":"2024-04-07T06:43:30.988966Z","iopub.status.idle":"2024-04-07T06:43:30.993945Z","shell.execute_reply.started":"2024-04-07T06:43:30.988930Z","shell.execute_reply":"2024-04-07T06:43:30.992661Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ndef preprocess_text(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word not in stop_words]\n    # Join tokens back into a string\n    preprocessed_text = ' '.join(tokens)\n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:55:52.788611Z","iopub.execute_input":"2024-04-07T06:55:52.789050Z","iopub.status.idle":"2024-04-07T06:55:52.870527Z","shell.execute_reply.started":"2024-04-07T06:55:52.789015Z","shell.execute_reply":"2024-04-07T06:55:52.869543Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Define preprocess_text function\ndef preprocess_text(text):\n    # Convert text to lowercase if it's a string\n    if isinstance(text, str):\n        text = text.lower()\n        # Remove special characters and digits\n        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    return text\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Apply preprocess_text function to each element of 'text' column\nX_train = df['text'].apply(preprocess_text)\ny_train = df['textID']  # Assuming 'textID' is the column name for the target labels\n\nvocab_size = 10000  \nmax_length = 100    ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:58:34.160626Z","iopub.execute_input":"2024-04-07T06:58:34.161104Z","iopub.status.idle":"2024-04-07T06:58:34.326262Z","shell.execute_reply.started":"2024-04-07T06:58:34.161052Z","shell.execute_reply":"2024-04-07T06:58:34.324581Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Assuming 'data' is the DataFrame loaded from the CSV\nle = LabelEncoder()\ndf['target'] = le.fit_transform(df['text'])\n\n# Convert text column to strings\ndf['text'] = df['text'].astype(str)\n\n# Convert text to sequences and pad sequences\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=8))  # Removed input_length parameter\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(padded_sequences, df['target'], epochs=10, batch_size=32, validation_split=0.2)\n\n# Summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:04:32.461636Z","iopub.execute_input":"2024-04-07T07:04:32.462129Z","iopub.status.idle":"2024-04-07T07:04:55.408953Z","shell.execute_reply.started":"2024-04-07T07:04:32.462089Z","shell.execute_reply":"2024-04-07T07:04:55.407501Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 1.8864e-05 - loss: -51905144.0000 - val_accuracy: 0.0000e+00 - val_loss: -517450784.0000\nEpoch 2/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 5.0901e-05 - loss: -827775872.0000 - val_accuracy: 0.0000e+00 - val_loss: -1999462912.0000\nEpoch 3/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 4.9116e-05 - loss: -2514158592.0000 - val_accuracy: 0.0000e+00 - val_loss: -4243735296.0000\nEpoch 4/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 1.3868e-04 - loss: -4947546112.0000 - val_accuracy: 0.0000e+00 - val_loss: -7128005632.0000\nEpoch 5/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.1330e-05 - loss: -7943320064.0000 - val_accuracy: 0.0000e+00 - val_loss: -10585164800.0000\nEpoch 6/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 2.8763e-05 - loss: -11560315904.0000 - val_accuracy: 0.0000e+00 - val_loss: -14567382016.0000\nEpoch 7/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 2.3489e-05 - loss: -15769712640.0000 - val_accuracy: 0.0000e+00 - val_loss: -19040325632.0000\nEpoch 8/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 3.5589e-05 - loss: -20263421952.0000 - val_accuracy: 0.0000e+00 - val_loss: -23998713856.0000\nEpoch 9/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 4.0463e-05 - loss: -25540667392.0000 - val_accuracy: 0.0000e+00 - val_loss: -29423192064.0000\nEpoch 10/10\n\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 5.8360e-05 - loss: -31047516160.0000 - val_accuracy: 0.0000e+00 - val_loss: -35305615360.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_13\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │        \u001b[38;5;34m80,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m800\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │           \u001b[38;5;34m801\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m242,405\u001b[0m (946.90 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">242,405</span> (946.90 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m161,604\u001b[0m (631.27 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,604</span> (631.27 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of y_train:\", y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:05:01.023647Z","iopub.execute_input":"2024-04-07T07:05:01.024098Z","iopub.status.idle":"2024-04-07T07:05:01.031693Z","shell.execute_reply.started":"2024-04-07T07:05:01.024060Z","shell.execute_reply":"2024-04-07T07:05:01.029909Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Shape of X_train: (27481,)\nShape of y_train: (27481,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Drop rows with NaN values in the 'text' column\ndf = df.dropna(subset=['text'])\n\n# Convert non-string values to string\ndf['text'] = df['text'].astype(str)\n\n# Tokenize and pad sequences for X_train\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df['text'])\nX_train_seq = tokenizer.texts_to_sequences(df['text'])\nX_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:13:51.217382Z","iopub.execute_input":"2024-04-07T07:13:51.218388Z","iopub.status.idle":"2024-04-07T07:13:52.302059Z","shell.execute_reply.started":"2024-04-07T07:13:51.218354Z","shell.execute_reply":"2024-04-07T07:13:52.300672Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Assuming you have defined vocab_size and max_length\nvocab_size = 10000\nmax_length = 100\n\n# Assuming you have already split your data into training and testing sets\n# Convert text to sequences and pad sequences for X_test\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(X_test)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:16:33.373965Z","iopub.execute_input":"2024-04-07T07:16:33.374412Z","iopub.status.idle":"2024-04-07T07:16:33.682051Z","shell.execute_reply.started":"2024-04-07T07:16:33.374374Z","shell.execute_reply":"2024-04-07T07:16:33.681121Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode the target labels\ny_test_encoded = label_encoder.fit_transform(y_test)\n\n# Now, evaluate the model with encoded labels\nloss, acc = model.evaluate(X_test_padded, y_test_encoded, verbose=0)\nprint('Test Accuracy: %.2f%%' % (acc * 100))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:18:47.144116Z","iopub.execute_input":"2024-04-07T07:18:47.144515Z","iopub.status.idle":"2024-04-07T07:18:47.632757Z","shell.execute_reply.started":"2024-04-07T07:18:47.144488Z","shell.execute_reply":"2024-04-07T07:18:47.631814Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Test Accuracy: 37.06%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Train word2vec Embedding","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming you have a CSV file with 'text' and 'sentiment' columns\ndf_pos = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\ndf_neg = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/test.csv', encoding='ISO-8859-1')\n\n# Concatenate the positive and negative dataframes\ndf = pd.concat([df_pos, df_neg], ignore_index=True)\n\n# Assuming 'text' column contains the text data and 'sentiment' column contains the labels\nsentences = df['text'].tolist()\nlabels = df['sentiment'].tolist()\n\nprint('Total training sentences:', len(sentences))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:21:35.694368Z","iopub.execute_input":"2024-04-07T07:21:35.694799Z","iopub.status.idle":"2024-04-07T07:21:35.793994Z","shell.execute_reply.started":"2024-04-07T07:21:35.694764Z","shell.execute_reply":"2024-04-07T07:21:35.793035Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Total training sentences: 32296\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom gensim.models import Word2Vec\nimport nltk\nnltk.download('punkt')\n\n# Define preprocess_text function if not defined\ndef preprocess_text(text):\n    # Implement your preprocessing steps here\n    # For example, lowercasing and removing punctuation\n    if pd.isnull(text):  # Check if the value is NaN\n        return ''  # Replace NaN with an empty string\n    text = text.lower()\n    text = text.strip()  # Remove leading and trailing whitespaces\n    # Additional preprocessing steps can be added here\n    return text\n\n# Read the CSV file containing the text data\ndf = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Apply preprocess_text function to each element of 'text' column\ndf['processed_text'] = df['text'].apply(preprocess_text)\n\n# Tokenize text into sentences\nsentences = [nltk.word_tokenize(text) for text in df['processed_text']]\n\n# Train Word2Vec model\nmodel = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n\n# Retrieve vocabulary from the model\nwords = list(model.wv.key_to_index.keys())\nprint('Vocabulary size:', len(words))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:25:59.639017Z","iopub.execute_input":"2024-04-07T07:25:59.639413Z","iopub.status.idle":"2024-04-07T07:26:06.081907Z","shell.execute_reply.started":"2024-04-07T07:25:59.639387Z","shell.execute_reply":"2024-04-07T07:26:06.081006Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nVocabulary size: 30708\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"# Sentiment Column\n\nimport pandas as pd\nfrom textblob import TextBlob\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Handle NaN values if necessary\ndata.dropna(subset=['sentiment'], inplace=True)\n\n# Calculating sentiment polarity for each review\ndata['Sentiment'] = data['sentiment'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n\n# Displaying the DataFrame with the sentiment column\nprint(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:32:45.181963Z","iopub.execute_input":"2024-04-07T07:32:45.182358Z","iopub.status.idle":"2024-04-07T07:32:48.337178Z","shell.execute_reply.started":"2024-04-07T07:32:45.182330Z","shell.execute_reply":"2024-04-07T07:32:48.335432Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"           textID                                               text  \\\n0      cb774db0d1                I`d have responded, if I were going   \n1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2      088c60f138                          my boss is bullying me...   \n3      9642c003ef                     what interview! leave me alone   \n4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n...           ...                                                ...   \n27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n27479  ed167662a5                         But it was worth it  ****.   \n27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n\n                                           selected_text sentiment  \\\n0                    I`d have responded, if I were going   neutral   \n1                                               Sooo SAD  negative   \n2                                            bullying me  negative   \n3                                         leave me alone  negative   \n4                                          Sons of ****,  negative   \n...                                                  ...       ...   \n27476                                             d lost  negative   \n27477                                      , don`t force  negative   \n27478                          Yay good for both of you.  positive   \n27479                         But it was worth it  ****.  positive   \n27480  All this flirting going on - The ATG smiles. Y...   neutral   \n\n      Time of Tweet Age of User      Country  Population -2020  \\\n0           morning        0-20  Afghanistan          38928346   \n1              noon       21-30      Albania           2877797   \n2             night       31-45      Algeria          43851044   \n3           morning       46-60      Andorra             77265   \n4              noon       60-70       Angola          32866272   \n...             ...         ...          ...               ...   \n27476         night       31-45        Ghana          31072940   \n27477       morning       46-60       Greece          10423054   \n27478          noon       60-70      Grenada            112523   \n27479         night      70-100    Guatemala          17915568   \n27480       morning        0-20       Guinea          13132795   \n\n       Land Area (Km²)  Density (P/Km²)  Sentiment  \n0             652860.0               60   0.000000  \n1              27400.0              105  -0.300000  \n2            2381740.0               18  -0.300000  \n3                470.0              164  -0.300000  \n4            1246700.0               26  -0.300000  \n...                ...              ...        ...  \n27476         227540.0              137  -0.300000  \n27477         128900.0               81  -0.300000  \n27478            340.0              331   0.227273  \n27479         107160.0              167   0.227273  \n27480         246000.0               53   0.000000  \n\n[27481 rows x 11 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In Sentiment column, sentiment analysis score showed that more negative statements . It mean people sad,angry and frustration in all situation recorded.","metadata":{}},{"cell_type":"code","source":"# Text Column\n\nimport pandas as pd\nfrom textblob import TextBlob\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/sentiment-analysis-dataset/train.csv', encoding='ISO-8859-1')\n\n# Handle NaN values if necessary\ndata.dropna(subset=['text'], inplace=True)\n\n# Calculating sentiment polarity for each review\ndata['Sentiment'] = data['text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n\n# Displaying the DataFrame with the sentiment column\nprint(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:33:44.995721Z","iopub.execute_input":"2024-04-07T07:33:44.996184Z","iopub.status.idle":"2024-04-07T07:33:50.319767Z","shell.execute_reply.started":"2024-04-07T07:33:44.996137Z","shell.execute_reply":"2024-04-07T07:33:50.318297Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"           textID                                               text  \\\n0      cb774db0d1                I`d have responded, if I were going   \n1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2      088c60f138                          my boss is bullying me...   \n3      9642c003ef                     what interview! leave me alone   \n4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n...           ...                                                ...   \n27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n27479  ed167662a5                         But it was worth it  ****.   \n27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n\n                                           selected_text sentiment  \\\n0                    I`d have responded, if I were going   neutral   \n1                                               Sooo SAD  negative   \n2                                            bullying me  negative   \n3                                         leave me alone  negative   \n4                                          Sons of ****,  negative   \n...                                                  ...       ...   \n27476                                             d lost  negative   \n27477                                      , don`t force  negative   \n27478                          Yay good for both of you.  positive   \n27479                         But it was worth it  ****.  positive   \n27480  All this flirting going on - The ATG smiles. Y...   neutral   \n\n      Time of Tweet Age of User      Country  Population -2020  \\\n0           morning        0-20  Afghanistan          38928346   \n1              noon       21-30      Albania           2877797   \n2             night       31-45      Algeria          43851044   \n3           morning       46-60      Andorra             77265   \n4              noon       60-70       Angola          32866272   \n...             ...         ...          ...               ...   \n27476         night       31-45        Ghana          31072940   \n27477       morning       46-60       Greece          10423054   \n27478          noon       60-70      Grenada            112523   \n27479         night      70-100    Guatemala          17915568   \n27480       morning        0-20       Guinea          13132795   \n\n       Land Area (Km²)  Density (P/Km²)  Sentiment  \n0             652860.0               60   0.000000  \n1              27400.0              105  -0.976562  \n2            2381740.0               18   0.000000  \n3                470.0              164   0.000000  \n4            1246700.0               26   0.000000  \n...                ...              ...        ...  \n27476         227540.0              137   0.000000  \n27477         128900.0               81   0.184091  \n27478            340.0              331   0.366667  \n27479         107160.0              167   0.300000  \n27480         246000.0               53   0.000000  \n\n[27480 rows x 11 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In text column,sentiment analysis score are more positive statement like people feel statisfied and happy","metadata":{}}]}
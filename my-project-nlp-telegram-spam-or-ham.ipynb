{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7902951,"sourceType":"datasetVersion","datasetId":4641655}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T05:46:28.906107Z","iopub.execute_input":"2024-04-07T05:46:28.906427Z","iopub.status.idle":"2024-04-07T05:46:30.099279Z","shell.execute_reply.started":"2024-04-07T05:46:28.906398Z","shell.execute_reply":"2024-04-07T05:46:30.098395Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/telegram-spam-or-ham/dataset.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# A Word-Level Neural Language Model And Generate Text ","metadata":{}},{"cell_type":"code","source":"# load text\nfilename = '/kaggle/input/telegram-spam-or-ham/dataset.csv'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.100853Z","iopub.execute_input":"2024-04-07T05:46:30.101238Z","iopub.status.idle":"2024-04-07T05:46:30.251373Z","shell.execute_reply.started":"2024-04-07T05:46:30.101211Z","shell.execute_reply":"2024-04-07T05:46:30.250619Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef load_doc_from_csv(file_path, column_name):\n    \"\"\"\n    Load document content from a CSV file.\n\n    Args:\n    - file_path (str): The path to the CSV file.\n    - column_name (str): The name of the column containing the document content.\n\n    Returns:\n    - str: Document content.\n    \"\"\"\n    try:\n        # Load CSV file\n        df = pd.read_csv(file_path)\n        # Extract document content from specified column\n        doc = df['text'].iloc[0]  # Assuming there's only one document in the CSV file\n        return doc\n    except FileNotFoundError:\n        print(\"File not found:\", file_path)\n        return None\n\n# Specify the file path and column name\nfile_path = '/kaggle/input/telegram-spam-or-ham/dataset.csv'  # Replace 'your_csv_file.csv' with the actual file path\ncolumn_name = 'text'  # Replace 'column_containing_document_content' with the actual column name\n\n# Load document\ndoc = load_doc_from_csv(file_path, column_name)\nif doc is not None:\n    print(doc[:200])  # Print the first 200 characters of the document","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.252556Z","iopub.execute_input":"2024-04-07T05:46:30.252842Z","iopub.status.idle":"2024-04-07T05:46:30.376379Z","shell.execute_reply.started":"2024-04-07T05:46:30.252819Z","shell.execute_reply":"2024-04-07T05:46:30.375500Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"naturally irresistible your corporate identity lt is really hard to recollect a company the market is full of suqgestions and the information isoverwhelminq but a good catchy logo stylish statlonery a\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n    \"\"\"\n    Clean and tokenize a document.\n\n    Args:\n    - doc (str): The document to be cleaned.\n\n    Returns:\n    - list: List of clean tokens.\n    \"\"\"\n    # replace '--' with a space ' '\n    doc = doc.replace('--', ' ')\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation and make lower case\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [word.translate(table).lower() for word in tokens if word.isalpha()]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.379133Z","iopub.execute_input":"2024-04-07T05:46:30.379725Z","iopub.status.idle":"2024-04-07T05:46:30.385450Z","shell.execute_reply.started":"2024-04-07T05:46:30.379677Z","shell.execute_reply":"2024-04-07T05:46:30.384363Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Clean document\ntokens = clean_doc(doc)\n\n# Print the first 200 tokens\nprint(\"First 200 tokens:\")\nprint(tokens[:200])\n\n# Print total number of tokens and unique tokens\ntotal_tokens = len(tokens)\nunique_tokens = len(set(tokens))\nprint('Total Tokens:', total_tokens)\nprint('Unique Tokens:', unique_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.386528Z","iopub.execute_input":"2024-04-07T05:46:30.386850Z","iopub.status.idle":"2024-04-07T05:46:30.396589Z","shell.execute_reply.started":"2024-04-07T05:46:30.386825Z","shell.execute_reply":"2024-04-07T05:46:30.395772Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"First 200 tokens:\n['naturally', 'irresistible', 'your', 'corporate', 'identity', 'lt', 'is', 'really', 'hard', 'to', 'recollect', 'a', 'company', 'the', 'market', 'is', 'full', 'of', 'suqgestions', 'and', 'the', 'information', 'isoverwhelminq', 'but', 'a', 'good', 'catchy', 'logo', 'stylish', 'statlonery', 'and', 'outstanding', 'website', 'will', 'make', 'the', 'task', 'much', 'easier', 'we', 'do', 'not', 'promise', 'that', 'havinq', 'ordered', 'a', 'iogo', 'your', 'company', 'will', 'automaticaily', 'become', 'a', 'world', 'ieader', 'it', 'isguite', 'ciear', 'that', 'without', 'good', 'products', 'effective', 'business', 'organization', 'and', 'practicable', 'aim', 'it', 'will', 'be', 'hotat', 'nowadays', 'market', 'but', 'we', 'do', 'promise', 'that', 'your', 'marketing', 'efforts', 'will', 'become', 'much', 'more', 'effective', 'here', 'is', 'the', 'list', 'of', 'clear', 'benefits', 'creativeness', 'hand', 'made', 'original', 'logos', 'specially', 'done', 'to', 'reflect', 'your', 'distinctive', 'company', 'image', 'convenience', 'logo', 'and', 'stationery', 'are', 'provided', 'in', 'all', 'formats', 'easy', 'to', 'use', 'content']\nTotal Tokens: 121\nUnique Tokens: 86\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the length of each sequence\nsequence_length = 50 + 1\n\n# Initialize a list to store sequences\nsequences = []\n\n# Generate sequences of tokens\nfor i in range(sequence_length, len(tokens)):\n    # Select sequence of tokens\n    sequence = tokens[i - sequence_length: i]\n    # Convert the sequence into a line\n    line = ' '.join(sequence)\n    # Store the line\n    sequences.append(line)\n\n# Print the total number of sequences\nprint('Total Sequences:', len(sequences))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.397650Z","iopub.execute_input":"2024-04-07T05:46:30.397945Z","iopub.status.idle":"2024-04-07T05:46:30.407290Z","shell.execute_reply.started":"2024-04-07T05:46:30.397923Z","shell.execute_reply":"2024-04-07T05:46:30.406390Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total Sequences: 70\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Train Language Model","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade jax jaxlib\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:30.408506Z","iopub.execute_input":"2024-04-07T05:46:30.408787Z","iopub.status.idle":"2024-04-07T05:46:53.083731Z","shell.execute_reply.started":"2024-04-07T05:46:30.408761Z","shell.execute_reply":"2024-04-07T05:46:53.082780Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (0.4.23)\nCollecting jax\n  Downloading jax-0.4.26-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (0.4.23.dev20240116)\nCollecting jaxlib\n  Downloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.2.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax) (1.11.4)\nDownloading jax-0.4.26-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl (78.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: jaxlib, jax\n  Attempting uninstall: jaxlib\n    Found existing installation: jaxlib 0.4.23.dev20240116\n    Uninstalling jaxlib-0.4.23.dev20240116:\n      Successfully uninstalled jaxlib-0.4.23.dev20240116\n  Attempting uninstall: jax\n    Found existing installation: jax 0.4.23\n    Uninstalling jax-0.4.23:\n      Successfully uninstalled jax-0.4.23\nSuccessfully installed jax-0.4.26 jaxlib-0.4.26\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom numpy import array\nfrom pickle import dump\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:46:53.085101Z","iopub.execute_input":"2024-04-07T05:46:53.085378Z","iopub.status.idle":"2024-04-07T05:47:05.908576Z","shell.execute_reply.started":"2024-04-07T05:46:53.085351Z","shell.execute_reply":"2024-04-07T05:47:05.907681Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-07 05:46:55.217350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-07 05:46:55.217445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-07 05:46:55.381857: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset from CSV file\nfile_path = '/kaggle/input/telegram-spam-or-ham/dataset.csv'\ndf = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:05.909645Z","iopub.execute_input":"2024-04-07T05:47:05.910155Z","iopub.status.idle":"2024-04-07T05:47:06.022196Z","shell.execute_reply.started":"2024-04-07T05:47:05.910128Z","shell.execute_reply":"2024-04-07T05:47:06.021172Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Filter spam and ham messages\nspam_df = df[df['text_type'] == 'spam']\nham_df = df[df['text_type'] == 'ham']","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:06.025438Z","iopub.execute_input":"2024-04-07T05:47:06.025792Z","iopub.status.idle":"2024-04-07T05:47:06.907290Z","shell.execute_reply.started":"2024-04-07T05:47:06.025763Z","shell.execute_reply":"2024-04-07T05:47:06.906079Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Combine spam and ham messages\ndf = pd.concat([spam_df, ham_df])","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:06.908523Z","iopub.execute_input":"2024-04-07T05:47:06.908878Z","iopub.status.idle":"2024-04-07T05:47:06.915566Z","shell.execute_reply.started":"2024-04-07T05:47:06.908850Z","shell.execute_reply":"2024-04-07T05:47:06.914683Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Extract text and text type columns\nlines = df['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:06.916746Z","iopub.execute_input":"2024-04-07T05:47:06.917108Z","iopub.status.idle":"2024-04-07T05:47:06.925555Z","shell.execute_reply.started":"2024-04-07T05:47:06.917072Z","shell.execute_reply":"2024-04-07T05:47:06.924499Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequences = tokenizer.texts_to_sequences(lines)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:06.926742Z","iopub.execute_input":"2024-04-07T05:47:06.927356Z","iopub.status.idle":"2024-04-07T05:47:09.000542Z","shell.execute_reply.started":"2024-04-07T05:47:06.927325Z","shell.execute_reply":"2024-04-07T05:47:08.999491Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Vocabulary size\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:09.001875Z","iopub.execute_input":"2024-04-07T05:47:09.002246Z","iopub.status.idle":"2024-04-07T05:47:09.006776Z","shell.execute_reply.started":"2024-04-07T05:47:09.002215Z","shell.execute_reply":"2024-04-07T05:47:09.005824Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Pad sequences to ensure consistent length\nmax_sequence_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:09.007871Z","iopub.execute_input":"2024-04-07T05:47:09.008136Z","iopub.status.idle":"2024-04-07T05:47:09.213414Z","shell.execute_reply.started":"2024-04-07T05:47:09.008113Z","shell.execute_reply":"2024-04-07T05:47:09.212640Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Load the dataset from CSV file\nfile_path = '/kaggle/input/telegram-spam-or-ham/dataset.csv'\ndf = pd.read_csv(file_path)\n\n# Assuming 'data' is the DataFrame loaded from the CSV\nle = LabelEncoder()\ndf['target'] = le.fit_transform(df['text'])\n\n# Define problem\nvocab_size = 10000  # Assuming 'word_index' is predefined, otherwise adjust this value\nmax_length = 100\n\n# Convert text to sequences and pad sequences\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=8))  # Removed input_length parameter\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(padded_sequences, df['target'], epochs=10, batch_size=32, validation_split=0.2)\n\n# Summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:47:09.214507Z","iopub.execute_input":"2024-04-07T05:47:09.214801Z","iopub.status.idle":"2024-04-07T05:47:25.470254Z","shell.execute_reply.started":"2024-04-07T05:47:09.214777Z","shell.execute_reply":"2024-04-07T05:47:25.469375Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m107/509\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: -109044.0703","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1712468835.415304      95 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -8615766.0000 - val_accuracy: 9.8280e-04 - val_loss: -152776400.0000\nEpoch 2/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -148941696.0000 - val_accuracy: 9.8280e-04 - val_loss: -614914368.0000\nEpoch 3/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -461622528.0000 - val_accuracy: 9.8280e-04 - val_loss: -1333794048.0000\nEpoch 4/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -926077120.0000 - val_accuracy: 9.8280e-04 - val_loss: -2270014720.0000\nEpoch 5/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -1517213312.0000 - val_accuracy: 9.8280e-04 - val_loss: -3399120896.0000\nEpoch 6/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -2236388864.0000 - val_accuracy: 9.8280e-04 - val_loss: -4702202368.0000\nEpoch 7/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -2985311744.0000 - val_accuracy: 9.8280e-04 - val_loss: -6174325248.0000\nEpoch 8/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -3900789760.0000 - val_accuracy: 9.8280e-04 - val_loss: -7799063040.0000\nEpoch 9/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -4901786624.0000 - val_accuracy: 9.8280e-04 - val_loss: -9574303744.0000\nEpoch 10/10\n\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: -6020096000.0000 - val_accuracy: 9.8280e-04 - val_loss: -11491467264.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m80,000\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m801\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m242,405\u001b[0m (946.90 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">242,405</span> (946.90 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,801\u001b[0m (315.63 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,801</span> (315.63 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m161,604\u001b[0m (631.27 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,604</span> (631.27 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Use Language Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom random import randint\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\n\n# Load cleaned text sequences from CSV file\ndf = pd.read_csv('/kaggle/input/telegram-spam-or-ham/dataset.csv')\nlines = df['text'].tolist()\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nvocab_size = len(tokenizer.word_index) + 1\n\n# Generate sequences\nsequences = tokenizer.texts_to_sequences(lines)\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n\n# Prepare input and output sequences\nX = sequences[:, :-1]\ny = sequences[:, -1]\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 50))\nmodel.add(LSTM(100))\nmodel.add(Dense(vocab_size, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X, y, epochs=50, verbose=2)\n\n# Function to generate sequence\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n    result = []\n    in_text = seed_text\n    for _ in range(n_words):\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat.argmax():\n                out_word = word\n                break\n        in_text += ' ' + out_word\n        result.append(out_word)\n    return ' '.join(result)\n\n# Select a seed text\nseed_text = lines[randint(0, len(lines))]\nprint(seed_text + '\\n')\n\n# Generate new text\ngenerated = generate_seq(model, tokenizer, max_length-1, seed_text, 50)\nprint(generated)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:58:58.787370Z","iopub.execute_input":"2024-04-07T05:58:58.787748Z","iopub.status.idle":"2024-04-07T06:19:18.566568Z","shell.execute_reply.started":"2024-04-07T05:58:58.787720Z","shell.execute_reply":"2024-04-07T06:19:18.565568Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Epoch 1/50\n636/636 - 27s - 42ms/step - accuracy: 0.0344 - loss: 9.1768\nEpoch 2/50\n636/636 - 23s - 36ms/step - accuracy: 0.0412 - loss: 7.8149\nEpoch 3/50\n636/636 - 23s - 36ms/step - accuracy: 0.0508 - loss: 7.2957\nEpoch 4/50\n636/636 - 23s - 36ms/step - accuracy: 0.0647 - loss: 6.8838\nEpoch 5/50\n636/636 - 23s - 36ms/step - accuracy: 0.0788 - loss: 6.5042\nEpoch 6/50\n636/636 - 23s - 36ms/step - accuracy: 0.0977 - loss: 6.1140\nEpoch 7/50\n636/636 - 23s - 36ms/step - accuracy: 0.1177 - loss: 5.7538\nEpoch 8/50\n636/636 - 23s - 36ms/step - accuracy: 0.1374 - loss: 5.4063\nEpoch 9/50\n636/636 - 23s - 36ms/step - accuracy: 0.1618 - loss: 5.0757\nEpoch 10/50\n636/636 - 23s - 36ms/step - accuracy: 0.1852 - loss: 4.7564\nEpoch 11/50\n636/636 - 23s - 36ms/step - accuracy: 0.2107 - loss: 4.4469\nEpoch 12/50\n636/636 - 23s - 36ms/step - accuracy: 0.2446 - loss: 4.1403\nEpoch 13/50\n636/636 - 23s - 36ms/step - accuracy: 0.2831 - loss: 3.8458\nEpoch 14/50\n636/636 - 41s - 65ms/step - accuracy: 0.3299 - loss: 3.5514\nEpoch 15/50\n636/636 - 23s - 36ms/step - accuracy: 0.3837 - loss: 3.2712\nEpoch 16/50\n636/636 - 23s - 36ms/step - accuracy: 0.4489 - loss: 2.9965\nEpoch 17/50\n636/636 - 23s - 36ms/step - accuracy: 0.5086 - loss: 2.7285\nEpoch 18/50\n636/636 - 23s - 36ms/step - accuracy: 0.5635 - loss: 2.4875\nEpoch 19/50\n636/636 - 23s - 36ms/step - accuracy: 0.6111 - loss: 2.2493\nEpoch 20/50\n636/636 - 41s - 65ms/step - accuracy: 0.6487 - loss: 2.0359\nEpoch 21/50\n636/636 - 23s - 36ms/step - accuracy: 0.6866 - loss: 1.8416\nEpoch 22/50\n636/636 - 23s - 36ms/step - accuracy: 0.7151 - loss: 1.6728\nEpoch 23/50\n636/636 - 23s - 36ms/step - accuracy: 0.7400 - loss: 1.5222\nEpoch 24/50\n636/636 - 23s - 36ms/step - accuracy: 0.7654 - loss: 1.3755\nEpoch 25/50\n636/636 - 23s - 36ms/step - accuracy: 0.7876 - loss: 1.2478\nEpoch 26/50\n636/636 - 23s - 36ms/step - accuracy: 0.8044 - loss: 1.1482\nEpoch 27/50\n636/636 - 23s - 36ms/step - accuracy: 0.8218 - loss: 1.0469\nEpoch 28/50\n636/636 - 23s - 36ms/step - accuracy: 0.8357 - loss: 0.9518\nEpoch 29/50\n636/636 - 23s - 36ms/step - accuracy: 0.8503 - loss: 0.8735\nEpoch 30/50\n636/636 - 23s - 36ms/step - accuracy: 0.8634 - loss: 0.8001\nEpoch 31/50\n636/636 - 23s - 36ms/step - accuracy: 0.8742 - loss: 0.7343\nEpoch 32/50\n636/636 - 23s - 36ms/step - accuracy: 0.8853 - loss: 0.6780\nEpoch 33/50\n636/636 - 23s - 36ms/step - accuracy: 0.8956 - loss: 0.6205\nEpoch 34/50\n636/636 - 23s - 36ms/step - accuracy: 0.9055 - loss: 0.5628\nEpoch 35/50\n636/636 - 23s - 36ms/step - accuracy: 0.9111 - loss: 0.5262\nEpoch 36/50\n636/636 - 23s - 36ms/step - accuracy: 0.9193 - loss: 0.4795\nEpoch 37/50\n636/636 - 23s - 36ms/step - accuracy: 0.9256 - loss: 0.4471\nEpoch 38/50\n636/636 - 23s - 36ms/step - accuracy: 0.9266 - loss: 0.4333\nEpoch 39/50\n636/636 - 23s - 36ms/step - accuracy: 0.9352 - loss: 0.3891\nEpoch 40/50\n636/636 - 23s - 36ms/step - accuracy: 0.9406 - loss: 0.3528\nEpoch 41/50\n636/636 - 23s - 36ms/step - accuracy: 0.9485 - loss: 0.3139\nEpoch 42/50\n636/636 - 23s - 36ms/step - accuracy: 0.9461 - loss: 0.3096\nEpoch 43/50\n636/636 - 23s - 36ms/step - accuracy: 0.9440 - loss: 0.3193\nEpoch 44/50\n636/636 - 23s - 36ms/step - accuracy: 0.9546 - loss: 0.2706\nEpoch 45/50\n636/636 - 23s - 36ms/step - accuracy: 0.9573 - loss: 0.2458\nEpoch 46/50\n636/636 - 41s - 65ms/step - accuracy: 0.9598 - loss: 0.2300\nEpoch 47/50\n636/636 - 23s - 36ms/step - accuracy: 0.9618 - loss: 0.2133\nEpoch 48/50\n636/636 - 23s - 36ms/step - accuracy: 0.9633 - loss: 0.2103\nEpoch 49/50\n636/636 - 23s - 36ms/step - accuracy: 0.9641 - loss: 0.2039\nEpoch 50/50\n636/636 - 23s - 36ms/step - accuracy: 0.9661 - loss: 0.1845\nwho do you want to be like? your role model: 1 sports person 2 actor 3 politician 1/ 2/ 3 send the name of your role model if the above does not apply\n\nglobalinvestment2 service 9584 service walks tc 👇👇👇 norm150ptone 👇👇👇👇👇👇👇👇👇👇 globalinvestment2 👇👇👇👇👇👇👇👇👇👇 millerbtctrade skip here here 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 👇 asp asp picasso asp com asp\n","output_type":"stream"}]}]}
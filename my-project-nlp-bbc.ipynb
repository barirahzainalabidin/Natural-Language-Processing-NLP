{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8028092,"sourceType":"datasetVersion","datasetId":4731581}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# load text\nfilename = '/kaggle/input/bbc-full-text-document-classification/bbc_data.csv'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:24:29.069691Z","iopub.execute_input":"2024-04-06T06:24:29.070060Z","iopub.status.idle":"2024-04-06T06:24:29.084250Z","shell.execute_reply.started":"2024-04-06T06:24:29.070022Z","shell.execute_reply":"2024-04-06T06:24:29.083338Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import necessary modules\n\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:26:57.478593Z","iopub.execute_input":"2024-04-06T06:26:57.478998Z","iopub.status.idle":"2024-04-06T06:26:59.753699Z","shell.execute_reply.started":"2024-04-06T06:26:57.478969Z","shell.execute_reply":"2024-04-06T06:26:59.752845Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_text(tokens):\n    # Remove remaining tokens that are not alphabetic\n    tokens = [word for word in tokens if word.isalpha()]\n    \n    # Filter out stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word.lower() not in stop_words]\n    \n    # Filter out short tokens\n    tokens = [word for word in tokens if len(word) > 1]\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:27:02.428077Z","iopub.execute_input":"2024-04-06T06:27:02.429064Z","iopub.status.idle":"2024-04-06T06:27:02.435436Z","shell.execute_reply.started":"2024-04-06T06:27:02.429021Z","shell.execute_reply":"2024-04-06T06:27:02.434193Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# load doc and add to vocab\ndef add_doc_to_vocab(filename, vocab):\n # load doc\n doc = load_doc(filename)\n # clean doc\n tokens = clean_doc(doc)\n # update counts\n vocab.update(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:27:25.938397Z","iopub.execute_input":"2024-04-06T06:27:25.939012Z","iopub.status.idle":"2024-04-06T06:27:25.943940Z","shell.execute_reply.started":"2024-04-06T06:27:25.938982Z","shell.execute_reply":"2024-04-06T06:27:25.942784Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# load all docs in a directory\ndef process_docs(directory, vocab, is_train):\n    \"\"\"\n    Load documents from a directory based on whether they are for training or not.\n\n    Args:\n    - directory: Path to the directory containing documents.\n    - vocab: Set or dictionary to store vocabulary.\n    - is_train: Boolean indicating whether the documents are for training or not.\n    \"\"\"\n    # walk through all files in the folder\n    for filename in listdir(directory):\n        # skip any reviews in the test set if is_train is True\n        if is_train and filename.startswith('cv9'):\n            continue\n        # skip any reviews not in the test set if is_train is False\n        if not is_train and not filename.startswith('cv9'):\n            continue\n        # create the full path of the file to open\n        path = os.path.join(directory, filename)\n        # add doc to vocab\n        add_doc_to_vocab(path, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:30:17.518525Z","iopub.execute_input":"2024-04-06T06:30:17.519276Z","iopub.status.idle":"2024-04-06T06:30:17.527342Z","shell.execute_reply.started":"2024-04-06T06:30:17.519238Z","shell.execute_reply":"2024-04-06T06:30:17.525954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport pandas as pd\n\n# Function to load documents from a CSV file into a vocabulary\ndef process_docs_from_csv(csv_file, text_column, vocab, is_train):\n    \"\"\"\n    Load documents from a CSV file and add them to a vocabulary.\n\n    Args:\n    - csv_file: Path to the CSV file containing text data.\n    - text_column: Name of the column containing text data.\n    - vocab: Counter object to store vocabulary.\n    - is_train: Boolean indicating whether the documents are for training or not.\n    \"\"\"\n    # Read CSV file using pandas\n    data = pd.read_csv(csv_file)\n\n    # Iterate through each row in the DataFrame\n    for index, row in data.iterrows():\n        # Extract text from the specified column\n        text = row[text_column]\n\n        # Tokenize the text (assuming words are separated by whitespace)\n        tokens = text.split()\n\n        # Update the vocabulary with the tokens\n        vocab.update(tokens)\n\n# Define vocab\nvocab = Counter()\n\n# Path to the CSV file and the column containing text data\ncsv_file = '/kaggle/input/bbc-full-text-document-classification/bbc_data.csv'\ntext_column = 'data'\n\n# Add all docs to vocab\nprocess_docs_from_csv(csv_file, text_column, vocab, True)\n\n# Print the size of the vocab\nprint(\"Size of the vocabulary:\", len(vocab))\n\n# Print the top words in the vocab\nprint(\"Top 50 words in the vocabulary:\")\nprint(vocab.most_common(50))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:36:26.559819Z","iopub.execute_input":"2024-04-06T06:36:26.560237Z","iopub.status.idle":"2024-04-06T06:36:27.075769Z","shell.execute_reply.started":"2024-04-06T06:36:26.560207Z","shell.execute_reply":"2024-04-06T06:36:27.074377Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Size of the vocabulary: 64151\nTop 50 words in the vocabulary:\n[('the', 44485), ('to', 24800), ('of', 19833), ('and', 17904), ('a', 17139), ('in', 16487), ('for', 8637), ('is', 8448), ('that', 7528), ('The', 7196), ('on', 7148), ('was', 5992), ('be', 5740), ('with', 5107), ('has', 4933), ('said', 4900), ('it', 4860), ('have', 4718), ('as', 4652), ('will', 4398), ('at', 4386), ('by', 4363), ('are', 4332), ('he', 4219), ('from', 3473), ('not', 3329), ('-', 3195), ('Mr', 2979), ('his', 2825), ('an', 2663), ('but', 2609), ('its', 2603), ('would', 2557), ('had', 2553), ('which', 2551), ('been', 2464), ('they', 2432), ('their', 2347), ('were', 2256), ('I', 2246), ('more', 2138), ('this', 2126), ('also', 2098), ('who', 2021), ('up', 1825), ('about', 1723), ('we', 1709), ('people', 1695), ('than', 1601), ('or', 1596)]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\n\n# Load the CSV file\ncsv_file = '/kaggle/input/bbc-full-text-document-classification/bbc_data.csv'\ndata = pd.read_csv(csv_file)\n\n# Extract text data from the 'data' column\ntrain_docs = data['data'].tolist()\n\n# Create a tokenizer object\ntokenizer = Tokenizer()\n# Fit the tokenizer on the training documents\ntokenizer.fit_on_texts(train_docs)\n# Encode the training documents to sequences of integers\nencoded_docs = tokenizer.texts_to_sequences(train_docs)\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(s.split()) for s in train_docs])  # Find the maximum length\nXtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')  # Pad sequences\n\n# Define training labels\n# Assuming 'label' column contains the labels for each document\nytrain = data['labels'].values\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:44:33.179588Z","iopub.execute_input":"2024-04-06T06:44:33.179975Z","iopub.status.idle":"2024-04-06T06:44:34.700623Z","shell.execute_reply.started":"2024-04-06T06:44:33.179942Z","shell.execute_reply":"2024-04-06T06:44:34.699544Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:44:47.182287Z","iopub.execute_input":"2024-04-06T06:44:47.182666Z","iopub.status.idle":"2024-04-06T06:44:47.188000Z","shell.execute_reply.started":"2024-04-06T06:44:47.182639Z","shell.execute_reply":"2024-04-06T06:44:47.186918Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Initialize Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(data['data'])\nword_index = tokenizer.word_index\n\n# Initialize LabelEncoder\nle = LabelEncoder()\ndata['target'] = le.fit_transform(data['data'])\n\n# Define problem\nvocab_size = len(word_index) + 1\nmax_length = 100\n\n# Pad sequences\npadded_sequences = pad_sequences(tokenizer.texts_to_sequences(data['data']), maxlen=max_length)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=8))  # Removed input_length parameter\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(padded_sequences, data['target'], epochs=10, batch_size=32, validation_split=0.2)\n\n# Summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:52:13.898661Z","iopub.execute_input":"2024-04-06T06:52:13.899075Z","iopub.status.idle":"2024-04-06T06:52:19.365253Z","shell.execute_reply.started":"2024-04-06T06:52:13.899041Z","shell.execute_reply":"2024-04-06T06:52:19.364197Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.0011 - loss: -113.9706 - val_accuracy: 0.0000e+00 - val_loss: -1839.3181\nEpoch 2/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.6602e-04 - loss: -5070.2280 - val_accuracy: 0.0000e+00 - val_loss: -25534.9863\nEpoch 3/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0505e-04 - loss: -38751.1523 - val_accuracy: 0.0000e+00 - val_loss: -95461.6641\nEpoch 4/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.6602e-04 - loss: -118854.4141 - val_accuracy: 0.0000e+00 - val_loss: -212611.2656\nEpoch 5/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0505e-04 - loss: -243019.2188 - val_accuracy: 0.0000e+00 - val_loss: -373628.9375\nEpoch 6/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 2.4946e-04 - loss: -403064.0000 - val_accuracy: 0.0000e+00 - val_loss: -577369.0625\nEpoch 7/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0011 - loss: -628182.1250 - val_accuracy: 0.0000e+00 - val_loss: -820746.6250\nEpoch 8/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 6.2196e-04 - loss: -853791.1250 - val_accuracy: 0.0000e+00 - val_loss: -1100116.8750\nEpoch 9/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.9245e-04 - loss: -1156861.3750 - val_accuracy: 0.0000e+00 - val_loss: -1417607.3750\nEpoch 10/10\n\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.5327e-04 - loss: -1468128.8750 - val_accuracy: 0.0000e+00 - val_loss: -1772013.3750\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │       \u001b[38;5;34m247,488\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m801\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">247,488</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m744,869\u001b[0m (2.84 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">744,869</span> (2.84 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m248,289\u001b[0m (969.88 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">248,289</span> (969.88 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m496,580\u001b[0m (1.89 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">496,580</span> (1.89 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/bbc-full-text-document-classification/bbc_data.csv')\n\n# Handle NaN values\ndata.dropna(inplace=True)  # Remove rows with NaN values\n\n# Split the data into features (X) and target (y)\nX = data['data']\ny = data['labels']  # Assuming 'labels' column contains the target labels\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Vectorize the text data\nvectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\nX_train_vec = vectorizer.fit_transform(X_train)\nX_test_vec = vectorizer.transform(X_test)\n\n# Train a Support Vector Machine (SVM) classifier\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(X_train_vec, y_train)\n\n# Evaluate the classifier\ny_pred = svm_classifier.predict(X_test_vec)\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T07:03:19.198651Z","iopub.execute_input":"2024-04-06T07:03:19.199483Z","iopub.status.idle":"2024-04-06T07:03:22.676672Z","shell.execute_reply.started":"2024-04-06T07:03:19.199449Z","shell.execute_reply":"2024-04-06T07:03:22.674481Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"               precision    recall  f1-score   support\n\n     business       0.95      0.95      0.95       103\nentertainment       0.99      0.96      0.98        84\n     politics       0.93      0.97      0.95        80\n        sport       1.00      0.97      0.98        98\n         tech       0.96      0.97      0.97        80\n\n     accuracy                           0.97       445\n    macro avg       0.97      0.97      0.97       445\n weighted avg       0.97      0.97      0.97       445\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/bbc-full-text-document-classification/bbc_data.csv')\n\n# Handle NaN values\ndata.dropna(inplace=True)  # Remove rows with NaN values\n\n# Split the data into features (X) and target (y)\nX = data['data']\ny = data['labels']  # Assuming 'labels' column contains the target labels\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tokenize the text data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode the labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train_padded, y_train_encoded, epochs=10, verbose=2)\n\n# Evaluate\nloss, acc = model.evaluate(X_test_padded, y_test_encoded, verbose=0)\nprint('Test Accuracy: %.2f%%' % (acc * 100))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T07:07:05.270228Z","iopub.execute_input":"2024-04-06T07:07:05.271438Z","iopub.status.idle":"2024-04-06T07:07:09.769916Z","shell.execute_reply.started":"2024-04-06T07:07:05.271386Z","shell.execute_reply":"2024-04-06T07:07:09.768676Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/10\n56/56 - 1s - 19ms/step - accuracy: 0.1753 - loss: 0.3762\nEpoch 2/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -4.4992e+00\nEpoch 3/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -2.4852e+01\nEpoch 4/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -7.2474e+01\nEpoch 5/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -1.4930e+02\nEpoch 6/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -2.5146e+02\nEpoch 7/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -3.7642e+02\nEpoch 8/10\n56/56 - 0s - 5ms/step - accuracy: 0.1697 - loss: -5.2540e+02\nEpoch 9/10\n56/56 - 0s - 4ms/step - accuracy: 0.1697 - loss: -6.9703e+02\nEpoch 10/10\n56/56 - 0s - 6ms/step - accuracy: 0.1697 - loss: -8.8720e+02\nTest Accuracy: 18.88%\n","output_type":"stream"}]},{"cell_type":"markdown","source":" 18.88% were classified correctly by the model.","metadata":{}},{"cell_type":"code","source":"# Extracting sentences from the 'data' column\nsentences = data['data'].tolist()\n\nprint('Total training sentences: %d' % len(sentences))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T07:10:22.678486Z","iopub.execute_input":"2024-04-06T07:10:22.679616Z","iopub.status.idle":"2024-04-06T07:10:22.684688Z","shell.execute_reply.started":"2024-04-06T07:10:22.679575Z","shell.execute_reply":"2024-04-06T07:10:22.683946Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Total training sentences: 2225\n","output_type":"stream"}]}]}
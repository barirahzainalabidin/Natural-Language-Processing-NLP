{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7913842,"sourceType":"datasetVersion","datasetId":4649648}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T07:44:43.786038Z","iopub.execute_input":"2024-04-07T07:44:43.786994Z","iopub.status.idle":"2024-04-07T07:44:44.924952Z","shell.execute_reply.started":"2024-04-07T07:44:43.786952Z","shell.execute_reply":"2024-04-07T07:44:44.923804Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sms-spam-detection-dataset/spam_sms.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# A Word-Level Neural Language Model And Generate Text","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef load_doc_from_csv(file_path, column_name):\n    \"\"\"\n    Load document content from a CSV file.\n\n    Args:\n    - file_path (str): The path to the CSV file.\n    - column_name (str): The name of the column containing the document content.\n\n    Returns:\n    - str: Document content.\n    \"\"\"\n    try:\n        # Load CSV file\n        df = pd.read_csv(file_path)\n        # Extract document content from specified column\n        doc = df['v2'].iloc[0]  # Assuming there's only one document in the CSV file\n        return doc\n    except FileNotFoundError:\n        print(\"File not found:\", file_path)\n        return None\n\n# Specify the file path and column name\nfile_path = '/kaggle/input/sms-spam-detection-dataset/spam_sms.csv'  # Replace 'your_csv_file.csv' with the actual file path\ncolumn_name = 'v2'  \n\n# Load document\ndoc = load_doc_from_csv(file_path, column_name)\nif doc is not None:\n    print(doc[:200])  # Print the first 200 characters of the document","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:46:16.634798Z","iopub.execute_input":"2024-04-07T07:46:16.635239Z","iopub.status.idle":"2024-04-07T07:46:16.662890Z","shell.execute_reply.started":"2024-04-07T07:46:16.635205Z","shell.execute_reply":"2024-04-07T07:46:16.661738Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\n\n# turn a doc into clean tokens\ndef clean_doc(doc):\n    \"\"\"\n    Clean and tokenize a document.\n\n    Args:\n    - doc (str): The document to be cleaned.\n\n    Returns:\n    - list: List of clean tokens.\n    \"\"\"\n    # replace '--' with a space ' '\n    doc = doc.replace('--', ' ')\n    # split into tokens by white space\n    tokens = doc.split()\n    # remove punctuation and make lower case\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [word.translate(table).lower() for word in tokens if word.isalpha()]\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:46:38.291313Z","iopub.execute_input":"2024-04-07T07:46:38.292062Z","iopub.status.idle":"2024-04-07T07:46:38.298623Z","shell.execute_reply.started":"2024-04-07T07:46:38.292029Z","shell.execute_reply":"2024-04-07T07:46:38.297145Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Clean document\ntokens = clean_doc(doc)\n\n# Print the first 200 tokens\nprint(\"First 200 tokens:\")\nprint(tokens[:200])\n\n# Print total number of tokens and unique tokens\ntotal_tokens = len(tokens)\nunique_tokens = len(set(tokens))\nprint('Total Tokens:', total_tokens)\nprint('Unique Tokens:', unique_tokens)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:46:56.046386Z","iopub.execute_input":"2024-04-07T07:46:56.047631Z","iopub.status.idle":"2024-04-07T07:46:56.054975Z","shell.execute_reply.started":"2024-04-07T07:46:56.047598Z","shell.execute_reply":"2024-04-07T07:46:56.053703Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"First 200 tokens:\n['go', 'until', 'jurong', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'cine', 'there', 'got', 'amore']\nTotal Tokens: 16\nUnique Tokens: 16\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the length of each sequence\nsequence_length = 50 + 1\n\n# Initialize a list to store sequences\nsequences = []\n\n# Generate sequences of tokens\nfor i in range(sequence_length, len(tokens)):\n    # Select sequence of tokens\n    sequence = tokens[i - sequence_length: i]\n    # Convert the sequence into a line\n    line = ' '.join(sequence)\n    # Store the line\n    sequences.append(line)\n\n# Print the total number of sequences\nprint('Total Sequences:', len(sequences))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:47:19.151404Z","iopub.execute_input":"2024-04-07T07:47:19.151782Z","iopub.status.idle":"2024-04-07T07:47:19.158937Z","shell.execute_reply.started":"2024-04-07T07:47:19.151756Z","shell.execute_reply":"2024-04-07T07:47:19.157970Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Total Sequences: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade jax jaxlib","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:47:59.116440Z","iopub.execute_input":"2024-04-07T07:47:59.116905Z","iopub.status.idle":"2024-04-07T07:48:24.162036Z","shell.execute_reply.started":"2024-04-07T07:47:59.116877Z","shell.execute_reply":"2024-04-07T07:48:24.160323Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (0.4.25)\nCollecting jax\n  Downloading jax-0.4.26-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.10/site-packages (0.4.25)\nCollecting jaxlib\n  Downloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.2.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax) (1.11.4)\nDownloading jax-0.4.26-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl (78.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: jaxlib, jax\n  Attempting uninstall: jaxlib\n    Found existing installation: jaxlib 0.4.25\n    Uninstalling jaxlib-0.4.25:\n      Successfully uninstalled jaxlib-0.4.25\n  Attempting uninstall: jax\n    Found existing installation: jax 0.4.25\n    Uninstalling jax-0.4.25:\n      Successfully uninstalled jax-0.4.25\nSuccessfully installed jax-0.4.26 jaxlib-0.4.26\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom numpy import array\nfrom pickle import dump\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:48:24.164465Z","iopub.execute_input":"2024-04-07T07:48:24.164914Z","iopub.status.idle":"2024-04-07T07:48:37.319245Z","shell.execute_reply.started":"2024-04-07T07:48:24.164872Z","shell.execute_reply":"2024-04-07T07:48:37.318040Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-07 07:48:26.382183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-07 07:48:26.382328: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-07 07:48:26.531495: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the dataset from CSV file\nfile_path = '/kaggle/input/sms-spam-detection-dataset/spam_sms.csv'\ndf = pd.read_csv(file_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:48:40.274367Z","iopub.execute_input":"2024-04-07T07:48:40.275029Z","iopub.status.idle":"2024-04-07T07:48:40.303974Z","shell.execute_reply.started":"2024-04-07T07:48:40.274996Z","shell.execute_reply":"2024-04-07T07:48:40.302694Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Filter spam and ham messages\nspam_df = df[df['v2'] == 'spam']\nham_df = df[df['v2'] == 'ham']","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:49:32.375150Z","iopub.execute_input":"2024-04-07T07:49:32.375591Z","iopub.status.idle":"2024-04-07T07:49:32.388410Z","shell.execute_reply.started":"2024-04-07T07:49:32.375562Z","shell.execute_reply":"2024-04-07T07:49:32.386754Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Combine spam and ham messages\ndf = pd.concat([spam_df, ham_df])","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:49:45.180156Z","iopub.execute_input":"2024-04-07T07:49:45.180776Z","iopub.status.idle":"2024-04-07T07:49:45.185784Z","shell.execute_reply.started":"2024-04-07T07:49:45.180745Z","shell.execute_reply":"2024-04-07T07:49:45.184898Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Extract text and text type columns\nlines = df['v2'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:50:09.687203Z","iopub.execute_input":"2024-04-07T07:50:09.688067Z","iopub.status.idle":"2024-04-07T07:50:09.693556Z","shell.execute_reply.started":"2024-04-07T07:50:09.688025Z","shell.execute_reply":"2024-04-07T07:50:09.692440Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequences = tokenizer.texts_to_sequences(lines)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:50:24.012702Z","iopub.execute_input":"2024-04-07T07:50:24.013135Z","iopub.status.idle":"2024-04-07T07:50:24.019288Z","shell.execute_reply.started":"2024-04-07T07:50:24.013068Z","shell.execute_reply":"2024-04-07T07:50:24.017911Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Vocabulary size\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:50:41.504573Z","iopub.execute_input":"2024-04-07T07:50:41.504957Z","iopub.status.idle":"2024-04-07T07:50:41.509994Z","shell.execute_reply.started":"2024-04-07T07:50:41.504931Z","shell.execute_reply":"2024-04-07T07:50:41.508771Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"> Use Language Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom random import randint\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\n\n# Load cleaned v2 sequences from CSV file\ndf = pd.read_csv('/kaggle/input/sms-spam-detection-dataset/spam_sms.csv')\nlines = df['v2'].tolist()\n\n# Tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nvocab_size = len(tokenizer.word_index) + 1\n\n# Generate sequences\nsequences = tokenizer.texts_to_sequences(lines)\nmax_length = max([len(seq) for seq in sequences])\nsequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n\n# Prepare input and output sequences\nX = sequences[:, :-1]\ny = sequences[:, -1]\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 50))\nmodel.add(LSTM(100))\nmodel.add(Dense(vocab_size, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X, y, epochs=50, verbose=2)\n\n# Function to generate sequence\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n    result = []\n    in_text = seed_text\n    for _ in range(n_words):\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre')\n        # predict probabilities for each word\n        yhat = model.predict(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat.argmax():\n                out_word = word\n                break\n        in_text += ' ' + out_word\n        result.append(out_word)\n    return ' '.join(result)\n\n# Select a seed text\nseed_text = lines[randint(0, len(lines))]\nprint(seed_text + '\\n')\n\n# Generate new text\ngenerated = generate_seq(model, tokenizer, max_length-1, seed_text, 50)\nprint(generated)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T07:54:04.276802Z","iopub.execute_input":"2024-04-07T07:54:04.277262Z","iopub.status.idle":"2024-04-07T08:24:14.508017Z","shell.execute_reply.started":"2024-04-07T07:54:04.277229Z","shell.execute_reply":"2024-04-07T08:24:14.506810Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/50\n175/175 - 26s - 149ms/step - accuracy: 0.0162 - loss: 7.9998\nEpoch 2/50\n175/175 - 41s - 233ms/step - accuracy: 0.0153 - loss: 7.0193\nEpoch 3/50\n175/175 - 41s - 235ms/step - accuracy: 0.0178 - loss: 6.8540\nEpoch 4/50\n175/175 - 23s - 133ms/step - accuracy: 0.0185 - loss: 6.7455\nEpoch 5/50\n175/175 - 41s - 235ms/step - accuracy: 0.0210 - loss: 6.6424\nEpoch 6/50\n175/175 - 41s - 233ms/step - accuracy: 0.0210 - loss: 6.5044\nEpoch 7/50\n175/175 - 41s - 234ms/step - accuracy: 0.0258 - loss: 6.3441\nEpoch 8/50\n175/175 - 42s - 237ms/step - accuracy: 0.0370 - loss: 6.1783\nEpoch 9/50\n175/175 - 41s - 232ms/step - accuracy: 0.0459 - loss: 6.0202\nEpoch 10/50\n175/175 - 41s - 234ms/step - accuracy: 0.0537 - loss: 5.8609\nEpoch 11/50\n175/175 - 24s - 135ms/step - accuracy: 0.0642 - loss: 5.6993\nEpoch 12/50\n175/175 - 41s - 234ms/step - accuracy: 0.0754 - loss: 5.5298\nEpoch 13/50\n175/175 - 41s - 233ms/step - accuracy: 0.0827 - loss: 5.3839\nEpoch 14/50\n175/175 - 24s - 135ms/step - accuracy: 0.0933 - loss: 5.2317\nEpoch 15/50\n175/175 - 41s - 233ms/step - accuracy: 0.1071 - loss: 5.0683\nEpoch 16/50\n175/175 - 41s - 235ms/step - accuracy: 0.1237 - loss: 4.9239\nEpoch 17/50\n175/175 - 41s - 233ms/step - accuracy: 0.1369 - loss: 4.7801\nEpoch 18/50\n175/175 - 41s - 234ms/step - accuracy: 0.1522 - loss: 4.6276\nEpoch 19/50\n175/175 - 24s - 136ms/step - accuracy: 0.1746 - loss: 4.4924\nEpoch 20/50\n175/175 - 24s - 136ms/step - accuracy: 0.1836 - loss: 4.3434\nEpoch 21/50\n175/175 - 24s - 135ms/step - accuracy: 0.2028 - loss: 4.2175\nEpoch 22/50\n175/175 - 24s - 135ms/step - accuracy: 0.2195 - loss: 4.0797\nEpoch 23/50\n175/175 - 41s - 232ms/step - accuracy: 0.2423 - loss: 3.9366\nEpoch 24/50\n175/175 - 24s - 135ms/step - accuracy: 0.2660 - loss: 3.7965\nEpoch 25/50\n175/175 - 41s - 233ms/step - accuracy: 0.2891 - loss: 3.6782\nEpoch 26/50\n175/175 - 41s - 236ms/step - accuracy: 0.3121 - loss: 3.5440\nEpoch 27/50\n175/175 - 23s - 133ms/step - accuracy: 0.3351 - loss: 3.4224\nEpoch 28/50\n175/175 - 41s - 237ms/step - accuracy: 0.3561 - loss: 3.3161\nEpoch 29/50\n175/175 - 41s - 232ms/step - accuracy: 0.3783 - loss: 3.1986\nEpoch 30/50\n175/175 - 41s - 232ms/step - accuracy: 0.4165 - loss: 3.0689\nEpoch 31/50\n175/175 - 23s - 133ms/step - accuracy: 0.4365 - loss: 2.9517\nEpoch 32/50\n175/175 - 41s - 232ms/step - accuracy: 0.4560 - loss: 2.8417\nEpoch 33/50\n175/175 - 24s - 135ms/step - accuracy: 0.4865 - loss: 2.7400\nEpoch 34/50\n175/175 - 41s - 232ms/step - accuracy: 0.5054 - loss: 2.6356\nEpoch 35/50\n175/175 - 41s - 235ms/step - accuracy: 0.5251 - loss: 2.5232\nEpoch 36/50\n175/175 - 23s - 134ms/step - accuracy: 0.5537 - loss: 2.4253\nEpoch 37/50\n175/175 - 41s - 235ms/step - accuracy: 0.5716 - loss: 2.3253\nEpoch 38/50\n175/175 - 23s - 133ms/step - accuracy: 0.5852 - loss: 2.2475\nEpoch 39/50\n175/175 - 41s - 235ms/step - accuracy: 0.6104 - loss: 2.1415\nEpoch 40/50\n175/175 - 41s - 235ms/step - accuracy: 0.6323 - loss: 2.0401\nEpoch 41/50\n175/175 - 41s - 233ms/step - accuracy: 0.6508 - loss: 1.9684\nEpoch 42/50\n175/175 - 41s - 233ms/step - accuracy: 0.6696 - loss: 1.8758\nEpoch 43/50\n175/175 - 23s - 132ms/step - accuracy: 0.6931 - loss: 1.7911\nEpoch 44/50\n175/175 - 41s - 236ms/step - accuracy: 0.7064 - loss: 1.7146\nEpoch 45/50\n175/175 - 41s - 234ms/step - accuracy: 0.7182 - loss: 1.6352\nEpoch 46/50\n175/175 - 41s - 234ms/step - accuracy: 0.7365 - loss: 1.5737\nEpoch 47/50\n175/175 - 41s - 233ms/step - accuracy: 0.7531 - loss: 1.4934\nEpoch 48/50\n175/175 - 41s - 236ms/step - accuracy: 0.7669 - loss: 1.4209\nEpoch 49/50\n175/175 - 41s - 233ms/step - accuracy: 0.7818 - loss: 1.3514\nEpoch 50/50\n175/175 - 41s - 233ms/step - accuracy: 0.7918 - loss: 1.2844\nHow much u trying to get?\n\nthere doing out enough tomorrow sleep ok lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor lor\n","output_type":"stream"}]}]}
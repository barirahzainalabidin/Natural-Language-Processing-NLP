{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7742875,"sourceType":"datasetVersion","datasetId":4524732}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T08:08:59.740616Z","iopub.execute_input":"2024-04-06T08:08:59.741057Z","iopub.status.idle":"2024-04-06T08:09:01.159837Z","shell.execute_reply.started":"2024-04-06T08:08:59.741021Z","shell.execute_reply":"2024-04-06T08:09:01.158370Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemma_data_set_prompt_recover_1.csv\n/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemma_data_set_prompt_recover_2.csv\n/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"> Word Embedding Layers","metadata":{}},{"cell_type":"code","source":"# load text \n\nfilename = '/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv'\nfile = open(filename, 'rt')\ntext = file.read()\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:10:38.286052Z","iopub.execute_input":"2024-04-06T08:10:38.286750Z","iopub.status.idle":"2024-04-06T08:10:38.347678Z","shell.execute_reply.started":"2024-04-06T08:10:38.286714Z","shell.execute_reply":"2024-04-06T08:10:38.346227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from numpy import array\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:22:52.136398Z","iopub.execute_input":"2024-04-06T08:22:52.136952Z","iopub.status.idle":"2024-04-06T08:22:52.144807Z","shell.execute_reply.started":"2024-04-06T08:22:52.136882Z","shell.execute_reply":"2024-04-06T08:22:52.143233Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define documents\ndocs = [\n    'Hey Suzann! Just wanted to let you know that Grandma and Grandpa are coming over for dinner tonight',\n    'Dear Suzann, I hope this message finds you well. I am writing to inform you that your grandparents ...',\n    'Amidst an ornate room, Amy\\'s fingers gracefully danced across the intricate strings of her zither.',\n    'Betty\\'s peony bloomed exuberantly, rendering her garden with a symphony of hues, meticulously distri...',\n    'The cornet is a brass instrument resembling a trumpet, invented by Michael in the early 19th century...',\n    'Like the ebb and flow of the turbulent sea, dysfunction paints a canvas of shadows and light. While ...',\n    'Teresa\\'s heart raced as she lay on the operating table, a wave of trepidation washing over her.',\n    'Jane and her friends set sail on their yawl, the Sea Breeze, for a day of adventure.',\n    'Today, Angela and I explored the world of gastronomy at a local food festival',\n    'Title: The Bronco: A Sustainable Trailblazer The Bronco, an iconic American off-road vehicle by For...'\n]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:23:32.699369Z","iopub.execute_input":"2024-04-06T08:23:32.700529Z","iopub.status.idle":"2024-04-06T08:23:32.708251Z","shell.execute_reply.started":"2024-04-06T08:23:32.700474Z","shell.execute_reply":"2024-04-06T08:23:32.706837Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Integer encode the documents\nvocab_size = 50\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)\n\n# Pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:25:55.460873Z","iopub.execute_input":"2024-04-06T08:25:55.461330Z","iopub.status.idle":"2024-04-06T08:25:55.470832Z","shell.execute_reply.started":"2024-04-06T08:25:55.461299Z","shell.execute_reply":"2024-04-06T08:25:55.469164Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[[13, 7, 18, 16, 30, 34, 27, 34, 22, 39, 21, 7, 42, 7, 14, 1, 40, 10], [38, 7, 17, 32, 13, 11, 19, 27, 43, 17, 15, 12, 30, 35, 27, 22, 18, 10], [15, 43, 47, 25, 25, 8, 43, 46, 30, 3, 24, 16, 31, 40, 32], [21, 4, 15, 18, 29, 40, 22, 33, 4, 42, 31, 41, 18, 47], [3, 2, 10, 4, 15, 6, 45, 4, 40, 23, 46, 39, 41, 3, 16, 34, 12], [4, 3, 27, 21, 41, 31, 3, 44, 13, 34, 15, 4, 15, 31, 41, 21, 5, 39], [34, 44, 42, 19, 43, 4, 48, 3, 6, 1, 4, 12, 31, 30, 44, 14, 40], [47, 21, 40, 16, 34, 41, 48, 14, 7, 3, 13, 31, 1, 4, 37, 31, 29], [5, 20, 21, 17, 35, 3, 40, 31, 18, 39, 4, 34, 6, 30], [28, 3, 38, 4, 15, 26, 3, 38, 43, 27, 22, 25, 22, 23, 46, 1]]\n[[14  1 40 10]\n [27 22 18 10]\n [16 31 40 32]\n [31 41 18 47]\n [ 3 16 34 12]\n [41 21  5 39]\n [30 44 14 40]\n [ 4 37 31 29]\n [ 4 34  6 30]\n [22 23 46  1]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\n\n# Load the CSV file\ncsv_file = '/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv'\ndata = pd.read_csv(csv_file)\n\n# Extract text data from the 'data' column\ntrain_docs = data['rewritten_text'].tolist()\n\n# Create a tokenizer object\ntokenizer = Tokenizer()\n# Fit the tokenizer on the training documents\ntokenizer.fit_on_texts(train_docs)\n# Encode the training documents to sequences of integers\nencoded_docs = tokenizer.texts_to_sequences(train_docs)\n\n# Pad sequences to ensure uniform length\nmax_length = max([len(s.split()) for s in train_docs])  # Find the maximum length\nXtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')  # Pad sequences\n\n# Define training labels\n# Assuming 'label' column contains the labels for each document\nytrain = data['original_text'].values","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:33:06.032816Z","iopub.execute_input":"2024-04-06T08:33:06.033269Z","iopub.status.idle":"2024-04-06T08:33:06.434966Z","shell.execute_reply.started":"2024-04-06T08:33:06.033236Z","shell.execute_reply":"2024-04-06T08:33:06.433299Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:33:36.505824Z","iopub.execute_input":"2024-04-06T08:33:36.506234Z","iopub.status.idle":"2024-04-06T08:33:36.512539Z","shell.execute_reply.started":"2024-04-06T08:33:36.506205Z","shell.execute_reply":"2024-04-06T08:33:36.510970Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Define problem\nvocab_size = len(word_index) + 1\nmax_length = 100\n\n# Pad sequences\npadded_sequences = pad_sequences(tokenizer.texts_to_sequences(train_docs), maxlen=max_length)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=8))  # No need for input_length here\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(padded_sequences, np.array(data['target']), epochs=10, batch_size=32, validation_split=0.2)  \n\n# Summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:36:27.721091Z","iopub.execute_input":"2024-04-06T08:36:27.721567Z","iopub.status.idle":"2024-04-06T08:36:32.160259Z","shell.execute_reply.started":"2024-04-06T08:36:27.721531Z","shell.execute_reply":"2024-04-06T08:36:32.158700Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: -421.1597 - val_accuracy: 0.0028 - val_loss: -5554.8223\nEpoch 2/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -12321.8516 - val_accuracy: 0.0028 - val_loss: -52276.2930\nEpoch 3/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -73848.8281 - val_accuracy: 0.0028 - val_loss: -178374.5781\nEpoch 4/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -219712.8438 - val_accuracy: 0.0028 - val_loss: -391833.8750\nEpoch 5/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -448591.6875 - val_accuracy: 0.0028 - val_loss: -695440.5625\nEpoch 6/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -733531.8750 - val_accuracy: 0.0028 - val_loss: -1032253.0625\nEpoch 7/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -1076257.8750 - val_accuracy: 0.0028 - val_loss: -1440309.0000\nEpoch 8/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -1447939.5000 - val_accuracy: 0.0028 - val_loss: -1909091.0000\nEpoch 9/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: -2045647.3750 - val_accuracy: 0.0028 - val_loss: -2457606.5000\nEpoch 10/10\n\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: -2547939.7500 - val_accuracy: 0.0028 - val_loss: -3022771.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │       \u001b[38;5;34m100,656\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m800\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m801\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,656</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">800</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">801</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m304,373\u001b[0m (1.16 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">304,373</span> (1.16 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,457\u001b[0m (396.32 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,457</span> (396.32 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m202,916\u001b[0m (792.64 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">202,916</span> (792.64 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\n# Read the CSV file\ndata = pd.read_csv('/kaggle/input/llm-prompt-recovery-data-gemini-and-gemma/gemini_data_set_prompt_recover_3.csv')\n\n# Split the data into features (X) and target (y)\nX = data['rewritten_text']\ny = data['original_text']  # Assuming 'labels' column contains the target labels\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tokenize the text data\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\nX_train_sequences = tokenizer.texts_to_sequences(X_train)\nX_test_sequences = tokenizer.texts_to_sequences(X_test)\n\n# Pad the tokenized sequences\nmax_length = 100  # Define the maximum length of sequences\nX_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\nX_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode the labels\ny_train_encoded = label_encoder.fit_transform(y_train)\ny_test_encoded = label_encoder.transform(y_test)\n\n# Define the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=8))  # Remove input_length parameter\nmodel.add(Flatten())\nmodel.add(Dense(units=1, activation='sigmoid'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X_train_padded, y_train_encoded, epochs=10, verbose=2)\n\n# Evaluate\nloss, acc = model.evaluate(X_test_padded, y_test_encoded, verbose=0)\nprint('Test Accuracy: %.2f%%' % (acc * 100))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:39:52.052501Z","iopub.execute_input":"2024-04-06T08:39:52.053095Z","iopub.status.idle":"2024-04-06T08:39:55.735243Z","shell.execute_reply.started":"2024-04-06T08:39:52.053049Z","shell.execute_reply":"2024-04-06T08:39:55.733521Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/10\n46/46 - 1s - 25ms/step - accuracy: 0.0028 - loss: -2.2155e+02\nEpoch 2/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -3.3435e+03\nEpoch 3/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -1.3082e+04\nEpoch 4/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -3.4405e+04\nEpoch 5/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -6.6601e+04\nEpoch 6/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -1.1472e+05\nEpoch 7/10\n46/46 - 0s - 6ms/step - accuracy: 0.0028 - loss: -1.7453e+05\nEpoch 8/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -2.4976e+05\nEpoch 9/10\n46/46 - 0s - 6ms/step - accuracy: 0.0028 - loss: -3.3763e+05\nEpoch 10/10\n46/46 - 0s - 4ms/step - accuracy: 0.0028 - loss: -4.5105e+05\nTest Accuracy: 0.28%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"0.28% were classified correctly by the model","metadata":{}},{"cell_type":"markdown","source":"I dont get label = array in this dataframe so it not completed that code snippets","metadata":{}}]}